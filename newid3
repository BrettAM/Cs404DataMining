#!/usr/bin/env python3
import sys
import functools
import math

#1: hw says not to search the same variable twice, but continous values are
#       reused here
#     Solution: filter the variables that remain in datasets as id3 progresses?
#2: Empty splits could be asked
#     Solution: Classifiers know all possible output classes it can produce
#       entropy calculations discard Classifiers that result in empty classes

class Feature:
    def __init__(self, args):
        self.name = args[0]
        self.cont = args[1] == "continuous"
        self.validValues = args[1:] if not self.cont else []
    def __repr__(self):
        return self.name
    def __str__(self):
        return self.name

class LessThanEQ:
    def __init__(self, val):
        self.ltval = val
    def prompt(self, ans):
        return ("<=" if ans else ">") + self.ltval
    def __repr__(self):
        return "LessThanEQ({0.ltval})".format(self)
    def __str__(self):
        return "<={0.ltval}".format(self)
    def __call__(self,x):
        return x <= self.ltval

class Identity:
    def prompt(self, ans):
        return "="+ans
    def __repr__(self):
        return "Identity()"
    def __str__(self):
        return "="
    def __call__(self, x):
        return x
ident = Identity()

class Classifier:
    def __init__(self, key, by=ident):
        self.key = key
        self.by  = by
    def prompt(self, ans):
        return "{0}{1}:".format(self.key, self.by.prompt(ans))
    def __repr__(self):
        return "Classifier({0.key}, {0.by})".format(self)
    def __str__(self):
        return "{0.key}{0.by}".format(self)
    def __call__(self, caseDict):
        return self.by(caseDict.get(self.key, None))
    def __eq__(self, y):
        return self.key == y.key and self.by == y.by
    def filter(self, data):
        """ Filter this distinction out of the dataset """
        result = data.copy()
        del result[self.key]
        return result
    def possibleClasses(self):
        """ List of possible classes the classifier can produce """
        if self.by is ident:
            return self.key.validValues
        else:
            return [True, False]

def occurances(dicts, Classifier):
    """ map different classes to occurance counts in a list of dicts """
    values = [Classifier(d) for d in dicts]
    def countOccurance(d,v):
        d[v] += 1
        return d
    emptyDict = {key:0 for key in Classifier.possibleClasses()}
    return functools.reduce(countOccurance, values, emptyDict)

def entropy(occurances):
    """ Calculate the entropy represented by an list of occurance counts """
    total = sum(occurances)
    if total == 0:
        return math.inf
    rate = [c/total for c in occurances]
    return sum([-p*math.log2(p) for p in rate if p != 0])

def setEntropy(dataset, Classifier):
    """ Calculate the entropy in the data after dividing input by Classifier """
    return entropy(occurances(dataset, Classifier).values())

def splitDataset(dataset, Classifier):
    """ Return the resulting datasets when input is divided by Classifier """
    def appendCase(d,v):
        d[Classifier(v)].append(Classifier.filter(v))
        return d
    emptyDict = {key:[] for key in Classifier.possibleClasses()}
    return functools.reduce(appendCase, dataset, emptyDict)

def entropyGain(dataset, splitClass, entroClass):
    """ the entropy gain WRT entroClass when dataset is split by splitClass """
    splitsets = splitDataset(dataset, splitClass)
    gain = sum([len(s)*setEntropy(s, entroClass) for s in splitsets.values()])
    return gain

def findAllClasses(dataset):
    """ Return a list of all Classifiers applicable to this dataset """
    #ugly
    classes = []
    for f in dataset[0].keys():
        if not f.cont:
            classes.append(Classifier(f))
        else:
            values   = [data[f] for data in dataset]
            classes += [Classifier(f, LessThanEQ(v)) for v in values]
    return classes

def id3(dataset, solveFeature):
    if setEntropy(dataset, solveFeature) == 0.0:
        return (str(solveFeature(dataset[0])), [])
    classes = [c for c in findAllClasses(dataset) if not c == solveFeature]
    classes = [(entropyGain(dataset, c, solveFeature), c) for c in classes]
    classes.sort(key=lambda c: c[0])

    if len(classes) == 0:
        return ("Out of features", [])

    classifier = classes[0][1]
    print(classifier)
    for d in dataset:
        print("\t",classifier(d), d)

    # print(classes[0][1])
    sdata = splitDataset(dataset, classes[0][1])
    # print(dataset)
    # for k,v in sdata.items():
    #     print(k,v)
    # print()

    return (classes[0][1], [(k, id3(v,solveFeature)) for k,v in sdata.items()])

def displayTree(tree, ident):
    question = tree[0]
    if len(tree[1]) == 0: #ugly way to print tree leaves
        print(" "*ident + question)
    for (ans, nt) in tree[1]:
        print(" "*ident + question.prompt(ans))
        displayTree(nt, ident+4)

#input
featureCount = int(input())+1
featureLines = [input().split() for i in range(0, featureCount)]
features = [Feature(l) for l in featureLines]

data = [dict(zip(features, l.split())) for l in sys.stdin.readlines()]

result = id3(data, Classifier(features[-1]))
displayTree(result, 0)
